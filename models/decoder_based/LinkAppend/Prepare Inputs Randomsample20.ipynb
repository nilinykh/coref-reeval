{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4432bfe8-541a-49c5-9804-d94db65924a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "# taking stanford-parsed texts as tokens\n",
    "# why? because LinkAppend requires tokenised sentences and we want the results to be comparable to other models\n",
    "# plus, the parser is quite good\n",
    "basepath = '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d0c36a-de4a-4ba7-8728-e701edec03e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_from_text(text):\n",
    "    # regex specific to stanford output format\n",
    "    sentence_pattern = re.compile(r'Tokens:\\n(.*?)\\n\\nConstituency parse:', re.DOTALL)\n",
    "    token_pattern = re.compile(r'Text=([^\\s\\[]+)', re.DOTALL)\n",
    "\n",
    "    matches = sentence_pattern.findall(text)\n",
    "    reconstructed_sentences = []\n",
    "    for tokens_text in matches:\n",
    "        matches = token_pattern.findall(tokens_text)\n",
    "        sentence = ' '.join(matches)\n",
    "        reconstructed_sentences.append(sentence)\n",
    "    return reconstructed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927a18b6-169e-4656-81a6-fe1845e4872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_files = glob.glob(basepath + '*.out')\n",
    "parsed_files = [f for f in parsed_files if 'checkpoint' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8602e772-cc74-42f9-a8e8-7dd3871fa434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_3780.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_12405.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_8217.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_12403.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_6377.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_5987.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_8135.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_4689.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_4842.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_262.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_317.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_1829.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_1374.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_8676.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_5262.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_5649.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_4533.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_7596.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_10650.out',\n",
       " '/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/stanford-det/doc_12679.out']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9396568e-6650-4b12-bc03-ea2e072d2528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 8242.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_3780\n",
      "doc_12405\n",
      "doc_8217\n",
      "doc_12403\n",
      "doc_6377\n",
      "doc_5987\n",
      "doc_8135\n",
      "doc_4689\n",
      "doc_4842\n",
      "doc_262\n",
      "doc_317\n",
      "doc_1829\n",
      "doc_1374\n",
      "doc_8676\n",
      "doc_5262\n",
      "doc_5649\n",
      "doc_4533\n",
      "doc_7596\n",
      "doc_10650\n",
      "doc_12679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "vwp_linkappend_texts = []\n",
    "\n",
    "for file in tqdm.tqdm(parsed_files):\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "    sentences = extract_sentences_from_text(content)\n",
    "\n",
    "    processed_data = []\n",
    "    for i, sentence in enumerate(sentences, 1):\n",
    "        words = sentence.split()\n",
    "        tokens = [{'id': idx+1, 'text': word} for idx, word in enumerate(words)]\n",
    "        processed_data.append({\n",
    "            'id': i,\n",
    "            'speaker': None,\n",
    "            'text': sentence,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "\n",
    "    text_dict = {}\n",
    "    text_dict['id'] = 'doc_' + str(file.split('_')[-1].split('.out')[0])\n",
    "    text_dict['sentences'] = processed_data\n",
    "    text_dict['coref_chains'] = None\n",
    "    \n",
    "    vwp_linkappend_texts.append(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df20103e-c9c7-4827-8029-d53f97c0b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/home/xilini/coref/multimodal_coref/data/nikolai_test_out/randomsample_20/link-append/inputs_random20.json', 'w') as f:\n",
    "    json.dump(vwp_linkappend_texts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7ad50-456c-41c7-92ca-f03cbb1f1e16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9df2a77-23cc-4689-b9d6-9bad8b38879e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70a7a7-f1ba-49c9-af70-8485c5885dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkappend_env",
   "language": "python",
   "name": "linkappend_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
